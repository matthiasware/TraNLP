{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4802c39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7e924a",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e5ba1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformer.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea2675a",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_vocabulary = 10\n",
    "d_model = 16\n",
    "d_sentence = 8\n",
    "d_batch =  3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edf5b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(low=0, high=d_vocabulary, size=(d_batch, d_sentence))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e2191f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_embedding = Embedding(d_vocabulary, d_model, d_sentence)\n",
    "emb = model_embedding(x)\n",
    "#\n",
    "assert emb.shape == torch.Size((d_batch, d_sentence, d_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8788a2",
   "metadata": {},
   "source": [
    "### Padding Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b777771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer.utils import get_attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2489a1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask without heads\n",
    "d_vocabulary = 4\n",
    "d_batch = 3\n",
    "d_sentence = 5\n",
    "\n",
    "x = torch.randint(low=0, high=d_vocabulary, size=(d_batch, d_sentence))\n",
    "mask = get_attn_mask(x)\n",
    "\n",
    "assert torch.equal(mask, (x == 0).unsqueeze(1).repeat(1, d_sentence, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe72038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask with heads\n",
    "n_heads = 2\n",
    "d_vocabulary = 4\n",
    "d_batch = 3\n",
    "d_sentence = 5\n",
    "\n",
    "x = torch.randint(low=0, high=d_vocabulary, size=(d_batch, d_sentence))\n",
    "mask = get_attn_mask(x, n_heads=n_heads)\n",
    "\n",
    "assert mask.shape == torch.Size((d_batch, n_heads, d_sentence, d_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55447ba",
   "metadata": {},
   "source": [
    "### ScaledDotProductAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b5e78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformer.layers import ScaledDotProductAttention\n",
    "from transformer.utils import get_attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd06275d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# without dimension for heads\n",
    "#\n",
    "d_vocabulary = 7\n",
    "d_b = 4  # batch size\n",
    "d_k = 3  # dim of W_k\n",
    "d_v = 5  # dim of W_v\n",
    "d_l = 6  # length of sentences\n",
    "#\n",
    "Q = torch.rand((d_b, d_l, d_k))\n",
    "K = torch.rand((d_b, d_l, d_k))\n",
    "V = torch.rand((d_b, d_l, d_v))\n",
    "#\n",
    "x = torch.randint(low=0, high=d_vocabulary, size=(d_b, d_l))\n",
    "mask = get_attn_mask(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ed1d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sdpa = ScaledDotProductAttention(d_k)\n",
    "context, attn = model_sdpa(Q, K, V, mask)\n",
    "#\n",
    "assert context.shape == torch.Size((d_b, d_l, d_v))\n",
    "assert attn.shape == torch.Size((d_b, d_l, d_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823d7355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with dimensions for heads\n",
    "#\n",
    "d_vocabulary = 7\n",
    "d_b = 4  # batch size\n",
    "d_k = 3  # dim of W_k\n",
    "d_v = 5  # dim of W_v\n",
    "d_l = 6  # length of sentences\n",
    "n_h = 2  # number of heads\n",
    "#\n",
    "Q = torch.rand((d_b, n_h, d_l, d_k))\n",
    "K = torch.rand((d_b, n_h, d_l, d_k))\n",
    "V = torch.rand((d_b, n_h, d_l, d_v))\n",
    "#\n",
    "x = torch.randint(low=0, high=d_vocabulary, size=(d_b, d_l))\n",
    "mask = get_attn_mask(x, n_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eb45d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sdpa = ScaledDotProductAttention(d_k)\n",
    "context, attn = model_sdpa(Q, K, V, mask)\n",
    "#\n",
    "assert context.shape == torch.Size((d_b, n_h, d_l, d_v))\n",
    "assert attn.shape == torch.Size((d_b, n_h, d_l, d_l))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d262c9",
   "metadata": {},
   "source": [
    "### Multihead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd14d232",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer.layers import MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4df5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_m = 8\n",
    "d_v = 8 # must be equal to d_m so far. sorry, crappy cupling of modules ;)\n",
    "#\n",
    "d_k = 6\n",
    "n_h = 2\n",
    "d_l = 7\n",
    "d_b = 3\n",
    "#\n",
    "model_mha = MultiHeadAttention(d_m, d_k, d_v, n_h)\n",
    "#\n",
    "x = torch.randint(low=0, high=d_vocabulary, size=(d_b, d_l))\n",
    "mask = get_attn_mask(x)\n",
    "\n",
    "# random embedding\n",
    "emb = torch.rand((d_b, d_l, d_m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3519455a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output, attn = model_mha(emb, mask)\n",
    "#\n",
    "assert output.shape == torch.Size((d_b, d_l, d_v))\n",
    "assert attn.shape == torch.Size((d_b, n_h, d_l, d_l))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e8f0b8",
   "metadata": {},
   "source": [
    "### Position Wise Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa379d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer.layers import PoswiseFeedForwardNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b827b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_b = 1\n",
    "d_m = 3\n",
    "d_ff = 4\n",
    "d_l = 8\n",
    "#\n",
    "x = torch.rand((d_b, d_l, d_m))\n",
    "#\n",
    "model_pffn = PoswiseFeedForwardNet(d_m, d_ff)\n",
    "#\n",
    "out = model_pffn(x)\n",
    "#\n",
    "assert out.shape == torch.Size((d_b, d_l, d_m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b669cba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = 0.6\n",
    "v2 = 0.7\n",
    "#\n",
    "x = torch.rand((d_b, d_l, d_m))\n",
    "for i in range(d_l):\n",
    "    if i % 2 == 0:\n",
    "        x[0][i,:] = v1\n",
    "    else:\n",
    "        x[0][i,:] = v2\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6600ba55",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model_pffn(x)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc388be0",
   "metadata": {},
   "source": [
    "### EncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a6708c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v, n_heads, d_ff):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, d_k, d_v, n_heads)\n",
    "        self.pos_ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "        \n",
    "    def forward(self, x, attn_mask):\n",
    "        enc_outputs, attn = self.enc_self_attn(x, attn_mask)\n",
    "        enc_outputs = self.pos_ffn(enc_outputs)\n",
    "        return enc_outputs, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de220c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_el = EncoderLayer(config.d_model, config.d_k, config.d_v, config.n_heads, config.d_ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9872ad79",
   "metadata": {},
   "outputs": [],
   "source": [
    "out, attn = model_el.forward(x, attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9fbf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out.shape)\n",
    "print(attn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa407579",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
