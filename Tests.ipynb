{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e2a0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cee23f",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2005bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformer.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fec35da",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_vocabulary = 10\n",
    "d_model = 16\n",
    "d_sentence = 8\n",
    "d_batch =  3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2dc726",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(low=0, high=d_vocabulary, size=(d_batch, d_sentence))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810eb735",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_embedding = Embedding(d_vocabulary, d_model, d_sentence)\n",
    "emb = model_embedding(x)\n",
    "#\n",
    "assert emb.shape == torch.Size((d_batch, d_sentence, d_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7341c9",
   "metadata": {},
   "source": [
    "### Padding Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e146de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer.utils import get_attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1449dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask without heads\n",
    "d_vocabulary = 4\n",
    "d_batch = 3\n",
    "d_sentence = 5\n",
    "\n",
    "x = torch.randint(low=0, high=d_vocabulary, size=(d_batch, d_sentence))\n",
    "mask = get_attn_mask(x)\n",
    "\n",
    "assert torch.equal(mask, (x == 0).unsqueeze(1).repeat(1, d_sentence, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d87d7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask with heads\n",
    "n_heads = 2\n",
    "d_vocabulary = 4\n",
    "d_batch = 3\n",
    "d_sentence = 5\n",
    "\n",
    "x = torch.randint(low=0, high=d_vocabulary, size=(d_batch, d_sentence))\n",
    "mask = get_attn_mask(x, n_heads=n_heads)\n",
    "\n",
    "assert mask.shape == torch.Size((d_batch, d_heads, d_sentence, d_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55447ba",
   "metadata": {},
   "source": [
    "### ScaledDotProductAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d4ba59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformer.layers import ScaledDotProductAttention\n",
    "from transformer.utils import get_attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2769852e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# without dimension for heads\n",
    "#\n",
    "d_vocabulary = 7\n",
    "d_b = 4  # batch size\n",
    "d_k = 3  # dim of W_k\n",
    "d_v = 5  # dim of W_v\n",
    "d_l = 6  # length of sentences\n",
    "#\n",
    "Q = torch.rand((d_b, d_l, d_k))\n",
    "K = torch.rand((d_b, d_l, d_k))\n",
    "V = torch.rand((d_b, d_l, d_v))\n",
    "#\n",
    "x = torch.randint(low=0, high=d_vocabulary, size=(d_b, d_l))\n",
    "mask = get_attn_mask(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487e7de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sdpa = ScaledDotProductAttention(d_k)\n",
    "context, attn = model_sdpa(Q, K, V, mask)\n",
    "#\n",
    "assert context.shape == torch.Size((d_b, d_l, d_v))\n",
    "assert attn.shape == torch.Size((d_b, d_l, d_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58afee42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with dimensions for heads\n",
    "#\n",
    "d_vocabulary = 7\n",
    "d_b = 4  # batch size\n",
    "d_k = 3  # dim of W_k\n",
    "d_v = 5  # dim of W_v\n",
    "d_l = 6  # length of sentences\n",
    "n_h = 2  # number of heads\n",
    "#\n",
    "Q = torch.rand((d_b, n_h, d_l, d_k))\n",
    "K = torch.rand((d_b, n_h, d_l, d_k))\n",
    "V = torch.rand((d_b, n_h, d_l, d_v))\n",
    "#\n",
    "x = torch.randint(low=0, high=d_vocabulary, size=(d_b, d_l))\n",
    "mask = get_attn_mask(x, n_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7ba2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sdpa = ScaledDotProductAttention(d_k)\n",
    "context, attn = model_sdpa(Q, K, V, mask)\n",
    "#\n",
    "assert context.shape == torch.Size((d_b, n_h, d_l, d_v))\n",
    "assert attn.shape == torch.Size((d_b, n_h, d_l, d_l))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d262c9",
   "metadata": {},
   "source": [
    "### Multihead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb99f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer.layers import MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ce57a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_m = 8\n",
    "d_v = 8\n",
    "#\n",
    "d_k = 6\n",
    "n_h = 2\n",
    "d_l = 7\n",
    "d_b = 3\n",
    "#\n",
    "model_mha = MultiHeadAttention(d_m, d_k, d_v, n_h)\n",
    "#\n",
    "x = torch.randint(low=0, high=d_vocabulary, size=(d_b, d_l))\n",
    "mask = get_attn_mask(x)\n",
    "\n",
    "# random embedding\n",
    "emb = torch.rand((d_b, d_l, d_m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3519455a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output, attn = model_mha(emb, mask)\n",
    "#\n",
    "assert output.shape == torch.Size((d_b, d_l, d_v))\n",
    "assert attn.shape == torch.Size((d_b, n_h, d_l, d_l))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e8f0b8",
   "metadata": {},
   "source": [
    "### Position Wise Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb62202",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    \"\"\"\n",
    "    Paper Section 3.4, last paragraph notice that BERT used the GELU instead of RELU\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
    "        #return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.gelu = GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (b, l, m) -> (b, l, d_ff) -> (b, l, m)\n",
    "        out = self.fc1(x)\n",
    "        out = self.gelu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a6708c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v, n_heads, d_ff):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, d_k, d_v, n_heads)\n",
    "        self.pos_ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "        \n",
    "    def forward(self, x, attn_mask):\n",
    "        enc_outputs, attn = self.enc_self_attn(x, attn_mask)\n",
    "        enc_outputs = self.pos_ffn(enc_outputs)\n",
    "        return enc_outputs, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de220c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_el = EncoderLayer(config.d_model, config.d_k, config.d_v, config.n_heads, config.d_ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9872ad79",
   "metadata": {},
   "outputs": [],
   "source": [
    "out, attn = model_el.forward(x, attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9fbf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out.shape)\n",
    "print(attn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa407579",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
